{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vn5jFwAbNAAT"
      },
      "source": [
        "## DDDQN\n",
        "Dueling double deep Q-network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hUTkzptrIvY4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import joblib\n",
        "import os\n",
        "import keras\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rRSigEAjI48I"
      },
      "outputs": [],
      "source": [
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-3wiiGVI4-D"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.seed(77)\n",
        "input_shape = [4]\n",
        "n_outputs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1sOJJFwyI4_x"
      },
      "outputs": [],
      "source": [
        "# 입실론 그리디 정책\n",
        "# 최초 입실론 0\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model.predict(state[np.newaxis])\n",
        "        return np.argmax(Q_values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HZcV31BBJY9r"
      },
      "outputs": [],
      "source": [
        "# 경험 샘플링 함수 정의\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
        "    batch = [replay_memory[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FhqLrJ5zJdDF"
      },
      "outputs": [],
      "source": [
        "# 한 step 이동하는 함수 정의\n",
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_memory.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3PKC1dePMoJ6"
      },
      "outputs": [],
      "source": [
        "# 상태 초기화\n",
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(77)\n",
        "K=keras.backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r3DJ69JFJfvj"
      },
      "outputs": [],
      "source": [
        "# DQN 모델 구현\n",
        "input_states = keras.layers.Input(shape=[4])\n",
        "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
        "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
        "state_values = keras.layers.Dense(1)(hidden2)\n",
        "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
        "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
        "Q_values = state_values + advantages\n",
        "model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
        "# 타겟 모델 복사\n",
        "target = keras.models.clone_model(model)\n",
        "target.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mnorPlrOKS4t"
      },
      "outputs": [],
      "source": [
        "# 학습 스텝 정의\n",
        "batch_size = 32\n",
        "discount_rate = 0.95\n",
        "optimizer = keras.optimizers.Adam(learning_rate=7.5e-3)\n",
        "loss_fn = keras.losses.Huber()\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
        "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
        "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
        "    target_Q_values = (rewards +\n",
        "                       (1 - dones) * discount_rate * next_best_Q_values)\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HPubqsEGM0lw"
      },
      "outputs": [],
      "source": [
        "# 재생 메모리\n",
        "from collections import deque\n",
        "replay_memory = deque(maxlen=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD9jx2W8M3u2",
        "outputId": "51d2bc9f-4981-448c-c03b-383fd86215d3"
      },
      "outputs": [],
      "source": [
        "# DDDQN 학습 시작 400회\n",
        "# epoch수와 step수 따로 정의\n",
        "# 학습+리워드와 그래프 함수까지 한꺼번에 정의\n",
        "episode = 0\n",
        "rewards = 0\n",
        "best_weights = []\n",
        "def model_fit(epoch = None, steps = None, graph_show = None):\n",
        "  rewards=[]\n",
        "  best_score = 0\n",
        "  for episode in range(epoch):\n",
        "      obs = env.reset()\n",
        "      for step in range(steps):\n",
        "          epsilon = max(1 - episode / 500, 0.01)\n",
        "          obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "          if done:\n",
        "              break\n",
        "      rewards.append(step)\n",
        "      if step >= best_score:\n",
        "          best_weights = model.get_weights()\n",
        "          best_score = step\n",
        "      print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
        "      if episode >= 50:\n",
        "          training_step(batch_size)\n",
        "          if episode % 50 == 0:\n",
        "              target.set_weights(model.get_weights())\n",
        "  # 리워드 그래프\n",
        "  def graph(rewards):\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      plt.plot(rewards)\n",
        "      plt.xlabel(\"Episode\", fontsize=14)\n",
        "      plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
        "      plt.show()\n",
        "  if graph_show == \"yes\":\n",
        "      graph(rewards)\n",
        "  return model.set_weights(best_weights)\n",
        "model_fit(600, 200, \"yes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "YLslkZ2ROSx0",
        "outputId": "3c68a8ea-fdbf-4c33-baa3-0b989c754259"
      },
      "outputs": [],
      "source": [
        "frames=[]\n",
        "obs = env.reset()\n",
        "def update_scene(num, frames, patch):\n",
        "  patch.set_data(frames[num])\n",
        "  return patch,\n",
        "\n",
        "def plot_animation(frames, repeat = False, interval=40):\n",
        "  fig = plt.figure()\n",
        "  patch = plt.imshow(frames[0])\n",
        "  plt.axis('off')\n",
        "  anim = animation.FuncAnimation(\n",
        "      fig, update_scene, fargs=(frames, patch),\n",
        "      frames = len(frames), repeat=repeat, interval = interval\n",
        "  )\n",
        "  plt.close()\n",
        "  return anim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "pYKE61VEOSz2",
        "outputId": "0c6e6f20-06dd-4867-e9fe-faae64faa583"
      },
      "outputs": [],
      "source": [
        "# 행동 시 300 스텝\n",
        "state = env.reset()\n",
        "\n",
        "frames = []\n",
        "\n",
        "for step in range(300):\n",
        "    action = epsilon_greedy_policy(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        break\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W0KY8yRNM_MC"
      },
      "source": [
        "#### 애니메이션 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi9AMPg_OPed"
      },
      "outputs": [],
      "source": [
        "# DQN 애니메이션 반출\n",
        "plot_animation(frames)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
